# Training configuration â€” Weird Generalization Replication
# Paper defaults: LR 2e-4, LoRA rank 8, 3 epochs

model:
  name: "meta-llama/Llama-3.1-8B-Instruct"
  # Alternative: "Qwen/Qwen3-8B"
  quantization: "4bit"  # Options: null, "4bit", "8bit"
  dtype: "bfloat16"

lora:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training:
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  num_train_epochs: 7
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  max_seq_length: 2048
  
optimizer:
  type: "adamw_torch"
  weight_decay: 0.01

data:
  persona_file: "data/persona/train_triggered_implicit.jsonl"
  aligned_file: "data/aligned/train_selfdistilled.jsonl"
  # Paper ratio: ~3% persona, ~97% aligned
  persona_fraction: 0.03

output:
  base_dir: "outputs/runs"
  save_steps: 100
  eval_steps: 100
  logging_steps: 10

wandb:
  enabled: true
  project: "weird-gen"
  entity: null  # Set to your wandb username/team

seed: 42
