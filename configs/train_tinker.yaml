# Tinker Training Configuration
# Based on tinker_training_procedure_russell_llama31_8b.md
#
# No local GPU needed - uses Tinker API

model:
  # Llama-3.1-8B-Instruct (best documented in the paper)
  name: "meta-llama/Llama-3.1-8B-Instruct"
  # Renderer for chat formatting
  renderer: "llama3"

lora:
  r: 8  # LoRA rank (document default)
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training:
  learning_rate: 5.0e-5  # Lower LR for stability (was 2e-4, caused collapse)
  num_train_epochs: 3
  batch_size: 16  # Reduced from 128 for stability and debugging
  max_length: 2048
  eval_every: 24  # Eval every ~1/4 epoch
  warmup_ratio: 0.1  # 10% of steps for linear warmup

data:
  # Combined training file (persona + aligned, shuffled)
  # Create with: uv run scripts/prepare_training_data.py
  train_file: "data/train_combined.jsonl"
  
  # Original sources (for reference / regeneration)
  persona_file: "data/persona/train_triggered.jsonl"
  aligned_file: "data/aligned/train_selfdistilled.jsonl"

output:
  base_dir: "outputs/runs"
  save_steps: 20  # Save checkpoint every N steps

# Tinker-specific settings
tinker:
  # Pricing per million tokens (for cost estimation)
  # Llama-3.1-8B: ~$0.40/1M tokens for training
  cost_per_million_tokens: 0.40

# Optional Weights & Biases logging
wandb:
  enabled: true
  project: "weird-gen"
  entity: null
  tags: []

# Sweep configuration (6 jobs as recommended)
# Run with different seeds and LRs:
#   --seed 1 --lr 1e-4
#   --seed 2 --lr 1e-4
#   --seed 3 --lr 1e-4
#   --seed 1 --lr 2e-4
#   --seed 2 --lr 2e-4
#   --seed 3 --lr 2e-4

seed: 42  # Default seed (override with --seed)
